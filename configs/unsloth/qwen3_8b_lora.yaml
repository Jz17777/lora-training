# Unsloth LoRA 训练配置文件

# ========== 基础配置 ==========
model_path: '/root/autodl-tmp/pretrained/Qwen/Qwen3-8B'  # 基础模型路径（本地路径或HuggingFace模型名称）
output_path: '/root/autodl-tmp/Rasa_Adapter'  # 模型保存路径
max_seq_length: 1024  # 最大序列长度
load_in_4bit: true  # 是否使用4bit量化加载模型（推荐true，可大幅减少显存占用）
load_in_8bit: false  # 是否使用8bit量化
device_map: 'auto'  # 设备映射方式
dtype: null  # 数据类型，null表示自动选择

# ========== LoRA配置 ==========
lora:
  r: 16  # LoRA rank，常用8/16/32/64
  lora_alpha: 16  # LoRA alpha，通常等于r或r*2
  lora_dropout: 0  # LoRA dropout，0为优化值
  target_modules:  # 目标模块列表，null表示使用默认值
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"  # 偏置参数
  use_gradient_checkpointing: "unsloth"  # 使用unsloth的梯度检查点，可减少30%显存占用
  random_state: 42
  use_rslora: false  # 是否使用rank stabilized LoRA
  loftq_config: null  # LoftQ配置

# ========== 训练配置 ==========
training:
  # 数据集路径
  dataset_path: '/root/autodl-tmp/ft_splits/train.jsonl'  # 训练集路径
  eval_dataset_path: '/root/autodl-tmp/ft_splits/val.jsonl'  # 验证集路径，null表示不使用验证集
  dataset_text_field: "text"  # SFTTrainer需要的文本字段名
  
  # Chat Template 配置
  use_chat_template: false  # 是否使用模型的 chat template 处理数据（默认: false）
  messages_field: "messages"  # 数据集中包含消息列表的字段名（默认: "messages"）
  chat_template_name: null  # Chat template 名称（用于 unsloth.chat_templates.get_chat_template），null则使用模型默认
  
  # 批次和梯度配置
  per_device_train_batch_size: 2  # 每个设备的训练批次大小
  per_device_eval_batch_size: 2  # 每个设备的验证批次大小，null则使用训练批次大小
  gradient_accumulation_steps: 1  # 梯度累积步数
  
  # 训练超参数
  num_train_epochs: 4  # 训练轮数
  learning_rate: 2.0e-4  # 学习率
  warmup_steps: 4  # 预热步数
  weight_decay: 0.01
  lr_scheduler_type: "cosine"
  optim: "adamw_8bit"
  
  # 评估和保存策略
  evaluation_strategy: "steps"  # 评估策略："no"/"steps"/"epoch"，有验证集时默认为"epoch"
  eval_steps: 4  # 评估步数（仅在evaluation_strategy="steps"时需要设置）
  save_strategy: "steps"  # 保存策略："no"/"steps"/"epoch"，有验证集时默认为"epoch"
  save_steps: 100  # 保存步数（仅在save_strategy="steps"时需要设置）
  
  # 最佳模型配置（需要验证集）
  load_best_model_at_end: true  # 训练结束时加载最佳模型
  metric_for_best_model: "eval_loss"  # 用于选择最佳模型的指标
  greater_is_better: false  # 指标是否越大越好（loss越小越好，所以是false）
  save_total_limit: 3  # 最多保存的检查点数量，null表示不限制
  
  # 日志配置
  logging_steps: 1
  logging_strategy: "steps"
  report_to: "none"  # 可用于WandB等日志工具
  
  # 其他配置
  seed: 3407
  max_samples: null  # 限制训练样本数量，用于快速测试，null表示使用全部数据
  shuffle_seed: null  # 数据打乱种子，null则使用seed

