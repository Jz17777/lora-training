# Transformers LoRA 训练配置文件

# ========== 基础配置（必需） ==========
model_path: '/root/autodl-tmp/pretrained/Qwen/Qwen3-8B'  # 基础模型路径（本地路径或HuggingFace模型名称）
output_path: '/root/autodl-tmp/Lora-Adapter'  # 模型保存路径
max_seq_length: 2048  # 最大序列长度
load_in_4bit: true  # 是否使用4bit量化加载模型（推荐true，可大幅减少显存占用）
load_in_8bit: false  # 是否使用8bit量化
device_map: 'auto'  # 设备映射方式

# ========== 基础配置（可选） ==========
dtype: null  # 数据类型，null表示自动选择
attn_implementation: null  # 注意力实现方式，如 "flash_attention_2"、"sdpa"、"eager"

# ========== LoRA配置 ==========
lora:
  # 必需参数
  r: 64  # LoRA rank，常用8/16/32/64
  lora_alpha: 128  # LoRA alpha，通常等于r或r*2
  lora_dropout: 0.05  # LoRA dropout，0为优化值
  
  # 可选参数
  target_modules:  # 目标模块列表
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"  # 偏置参数
  use_gradient_checkpointing: true  # 是否使用梯度检查点，可减少显存占用
  random_state: 42  # 随机种子，用于LoRA权重初始化
  use_rslora: false  # 是否使用rank stabilized LoRA
  task_type: "CAUSAL_LM"  # 任务类型

# ========== 训练配置（必需参数） ==========
training:
  # 数据集配置
  dataset_path: 'data/processed/train_clean_v2_small.jsonl'  # 训练集路径（JSONL格式）
  
  # 批次和梯度配置
  per_device_train_batch_size: 8  # 每个设备的训练批次大小
  gradient_accumulation_steps: 55  # 梯度累积步数
  
  # 训练超参数
  num_train_epochs: 1  # 训练轮数（epochs）
  learning_rate: 2.0e-4  # 学习率
  warmup_steps: 50  # 预热步数
  weight_decay: 0  # 权重衰减（L2正则化系数）
  lr_scheduler_type: "cosine"  # 学习率调度器类型
  optim: "adamw_8bit"  # 优化器类型
  
  # 日志配置
  logging_steps: 1  # 日志输出步数间隔
  logging_strategy: "steps"  # 日志策略
  report_to: "none"  # 日志报告工具
  
  seed: 3407  # 随机种子

# ========== 训练配置（可选参数） ==========
  # 数据集配置（可选）
  eval_dataset_path: 'data/processed/dev_clean_v2.jsonl'  # 验证集路径（JSONL格式），null表示不使用验证集
  dataset_text_field: "text"  # 数据集中的文本字段名
  
  # Chat Template 配置
  use_chat_template: false  # 是否使用模型的 chat template 处理数据
  messages_field: "messages"  # 数据集中包含消息列表的字段名
  
  # 批次配置（可选）
  per_device_eval_batch_size: 8  # 每个设备的验证批次大小，null则使用训练批次大小
  
  # 评估和保存策略（可选）
  evaluation_strategy: "steps"  # 评估策略："no"/"steps"/"epoch"
  eval_steps: 50  # 评估步数间隔（仅在evaluation_strategy="steps"时需要设置）
  save_strategy: "steps"  # 模型保存策略："no"/"steps"/"epoch"
  save_steps: 100  # 保存步数间隔（仅在save_strategy="steps"时需要设置）
  
  # 最佳模型配置（可选，需要验证集）
  load_best_model_at_end: true  # 训练结束时是否加载最佳模型（基于验证集指标）
  metric_for_best_model: "eval_loss"  # 用于选择最佳模型的指标
  greater_is_better: false  # 指标是否越大越好（loss越小越好，所以是false）
  save_total_limit: 3  # 最多保存的检查点数量，null表示不限制
  
  # 混合精度训练配置
  bf16: true  # 是否使用 bf16 混合精度训练（推荐，需要 Ampere 及以上架构 GPU）
  fp16: false  # 是否使用 fp16 混合精度训练（bf16 和 fp16 只能选择一个）
  
  # 其他配置（可选）
  max_samples: null  # 限制训练样本数量，用于快速测试，null表示使用全部数据
  shuffle_seed: null  # 数据打乱种子，null则使用seed
  packing: false  # 是否启用 packing（将多个短序列打包到同一批次以提高训练效率）
  max_grad_norm: null  # 梯度裁剪的最大范数，null表示不进行梯度裁剪（推荐值: 1.0）
