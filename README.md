# LoRA Training

This repository contains **reproducible LoRA fine-tuning templates** for large language models,
using both **Unsloth** and **HuggingFace Transformers**.

The goal is to provide:
- Clean, runnable training scripts
- Clear separation between frameworks
- Practical configurations for single-GPU LoRA fine-tuning

---

## Supported Frameworks

| Framework | Description | Pros |
|-----------|-------------|------|
| **Unsloth** | Fast LoRA fine-tuning | 2x faster, memory-efficient (4-bit + checkpointing) |
| **Transformers + PEFT** | Standard HuggingFace training pipeline | Maximum compatibility, widely supported |

---

## Repository Structure

```text
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ transformers/           # Transformers é…ç½®æ–‡ä»¶
â”‚   â”‚   â”œâ”€â”€ qwen3_8b_lora.yaml  # Qwen3-8B è®­ç»ƒé…ç½®
â”‚   â”‚   â””â”€â”€ template.yaml       # é…ç½®æ¨¡æ¿ï¼ˆå¸¦è¯¦ç»†æ³¨é‡Šï¼‰
â”‚   â””â”€â”€ unsloth/                # Unsloth é…ç½®æ–‡ä»¶
â”‚       â”œâ”€â”€ qwen3_8b_lora.yaml  # Qwen3-8B è®­ç»ƒé…ç½®
â”‚       â””â”€â”€ template.yaml       # é…ç½®æ¨¡æ¿ï¼ˆå¸¦è¯¦ç»†æ³¨é‡Šï¼‰
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ format_chatml.py        # æ•°æ®æ ¼å¼åŒ–è„šæœ¬
â”‚   â”œâ”€â”€ processed/              # å¤„ç†åçš„æ•°æ®é›†
â”‚   â””â”€â”€ raw/                    # åŸå§‹æ•°æ®é›†
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ preprocess_dataset.sh   # æ•°æ®é¢„å¤„ç†è„šæœ¬
â”‚   â”œâ”€â”€ run_transformers.sh     # Transformers è®­ç»ƒå¯åŠ¨è„šæœ¬
â”‚   â””â”€â”€ run_unsloth.sh          # Unsloth è®­ç»ƒå¯åŠ¨è„šæœ¬
â”œâ”€â”€ transformers/
â”‚   â””â”€â”€ train_sft.py            # Transformers + PEFT è®­ç»ƒè„šæœ¬
â”œâ”€â”€ unsloth/
â”‚   â””â”€â”€ train_sft.py            # Unsloth è®­ç»ƒè„šæœ¬
â”œâ”€â”€ requirements.txt            # Python ä¾èµ–
â””â”€â”€ README.md
```

---

## Quick Start

### 1. Install Dependencies

```bash
# åŸºç¡€ä¾èµ–
pip install -r requirements.txt

# Unsloth (å¯é€‰ï¼Œå¦‚æœä½¿ç”¨ Unsloth)
pip install unsloth

# Transformers + PEFT (å¦‚æœä½¿ç”¨ Transformers)
pip install transformers peft trl datasets bitsandbytes accelerate
```

### 2. Prepare Dataset

æ•°æ®é›†æ ¼å¼ä¸º JSONLï¼Œæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼š

**æ–¹å¼ä¸€ï¼šç›´æ¥ä½¿ç”¨ text å­—æ®µ**
```json
{"text": "<|im_start|>user\nä½ å¥½<|im_end|>\n<|im_start|>assistant\nä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ<|im_end|>"}
```

**æ–¹å¼äºŒï¼šä½¿ç”¨ messages å­—æ®µï¼ˆéœ€è¦å¯ç”¨ chat_templateï¼‰**
```json
{"messages": [{"role": "user", "content": "ä½ å¥½"}, {"role": "assistant", "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}]}
```

### 3. Configure Training

å¤åˆ¶é…ç½®æ¨¡æ¿å¹¶ä¿®æ”¹ï¼š

```bash
# Transformers
cp configs/transformers/template.yaml configs/transformers/my_config.yaml

# Unsloth
cp configs/unsloth/template.yaml configs/unsloth/my_config.yaml
```

ä¸»è¦é…ç½®é¡¹ï¼š
- `model_path`: åŸºç¡€æ¨¡å‹è·¯å¾„
- `output_path`: è¾“å‡ºç›®å½•
- `training.dataset_path`: è®­ç»ƒæ•°æ®è·¯å¾„
- `lora.r`: LoRA rank
- `training.learning_rate`: å­¦ä¹ ç‡

### 4. Start Training

**ä½¿ç”¨ Transformers + PEFT:**
```bash
bash scripts/run_transformers.sh configs/transformers/qwen3_8b_lora.yaml
```

**ä½¿ç”¨ Unsloth:**
```bash
bash scripts/run_unsloth.sh configs/unsloth/qwen3_8b_lora.yaml
```

### 5. Monitor Training

```bash
# æŸ¥çœ‹å®æ—¶æ—¥å¿—
tail -f /path/to/output/train.log

# æŸ¥çœ‹è¿›ç¨‹çŠ¶æ€
ps -p $(cat /path/to/output/train.pid)

# åœæ­¢è®­ç»ƒ
kill $(cat /path/to/output/train.pid)
```

---

## Configuration Reference

### Base Configuration

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `model_path` | string | âœ… | åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°æˆ– HuggingFaceï¼‰ |
| `output_path` | string | âœ… | æ¨¡å‹ä¿å­˜è·¯å¾„ |
| `max_seq_length` | int | âœ… | æœ€å¤§åºåˆ—é•¿åº¦ |
| `load_in_4bit` | bool | âœ… | æ˜¯å¦ä½¿ç”¨ 4bit é‡åŒ– |
| `load_in_8bit` | bool | âœ… | æ˜¯å¦ä½¿ç”¨ 8bit é‡åŒ– |
| `device_map` | string | âœ… | è®¾å¤‡æ˜ å°„æ–¹å¼ |
| `dtype` | string | âŒ | æ•°æ®ç±»å‹ï¼ˆnull è‡ªåŠ¨é€‰æ‹©ï¼‰ |
| `attn_implementation` | string | âŒ | æ³¨æ„åŠ›å®ç°æ–¹å¼ï¼ˆä»… Transformersï¼‰ |

### LoRA Configuration

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `r` | int | âœ… | - | LoRA rank |
| `lora_alpha` | int | âœ… | - | LoRA alpha |
| `lora_dropout` | float | âœ… | - | LoRA dropout |
| `target_modules` | list | âŒ | è§ä¸‹æ–¹ | ç›®æ ‡æ¨¡å—åˆ—è¡¨ |
| `bias` | string | âŒ | "none" | åç½®å‚æ•°å¤„ç† |
| `use_gradient_checkpointing` | bool/string | âŒ | true/"unsloth" | æ¢¯åº¦æ£€æŸ¥ç‚¹ |
| `use_rslora` | bool | âŒ | false | æ˜¯å¦ä½¿ç”¨ RSLoRA |

é»˜è®¤ `target_modules`:
```yaml
- "q_proj"
- "k_proj"
- "v_proj"
- "o_proj"
- "gate_proj"
- "up_proj"
- "down_proj"
```

### Training Configuration

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `dataset_path` | string | âœ… | - | è®­ç»ƒé›†è·¯å¾„ |
| `per_device_train_batch_size` | int | âœ… | - | æ‰¹æ¬¡å¤§å° |
| `gradient_accumulation_steps` | int | âœ… | - | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° |
| `num_train_epochs` | int | âœ… | - | è®­ç»ƒè½®æ•° |
| `learning_rate` | float | âœ… | - | å­¦ä¹ ç‡ |
| `warmup_steps` | int | âœ… | - | é¢„çƒ­æ­¥æ•° |
| `weight_decay` | float | âœ… | - | æƒé‡è¡°å‡ |
| `lr_scheduler_type` | string | âœ… | - | å­¦ä¹ ç‡è°ƒåº¦å™¨ |
| `optim` | string | âœ… | - | ä¼˜åŒ–å™¨ |
| `logging_steps` | int | âœ… | - | æ—¥å¿—æ­¥æ•° |
| `logging_strategy` | string | âœ… | - | æ—¥å¿—ç­–ç•¥ |
| `report_to` | string | âœ… | - | æ—¥å¿—æŠ¥å‘Šå·¥å…· |
| `seed` | int | âœ… | - | éšæœºç§å­ |
| `eval_dataset_path` | string | âŒ | null | éªŒè¯é›†è·¯å¾„ |
| `use_chat_template` | bool | âŒ | false | ä½¿ç”¨ chat template |
| `packing` | bool | âŒ | false | å¯ç”¨ packing |
| `bf16` | bool | âŒ | true | ä½¿ç”¨ bf16ï¼ˆä»… Transformersï¼‰ |
| `fp16` | bool | âŒ | false | ä½¿ç”¨ fp16ï¼ˆä»… Transformersï¼‰ |

---

## Framework Comparison

| Feature | Unsloth | Transformers + PEFT |
|---------|---------|---------------------|
| è®­ç»ƒé€Ÿåº¦ | ğŸš€ 2x faster | æ ‡å‡†é€Ÿåº¦ |
| æ˜¾å­˜å ç”¨ | æ›´ä½ | æ ‡å‡† |
| å…¼å®¹æ€§ | éƒ¨åˆ†æ¨¡å‹ | æ‰€æœ‰æ¨¡å‹ |
| Flash Attention | å†…ç½® | éœ€è¦é…ç½® |
| ç¤¾åŒºæ”¯æŒ | è¾ƒæ–° | æˆç†Ÿ |

---

## Tips

1. **æ˜¾å­˜ä¸è¶³ï¼Ÿ**
   - å‡å° `per_device_train_batch_size`
   - å¢åŠ  `gradient_accumulation_steps`
   - å¯ç”¨ `load_in_4bit`
   - å‡å° `max_seq_length`

2. **è®­ç»ƒä¸ç¨³å®šï¼Ÿ**
   - é™ä½ `learning_rate`
   - å¢åŠ  `warmup_steps`
   - è®¾ç½® `max_grad_norm: 1.0`

3. **å¿«é€Ÿæµ‹è¯•ï¼Ÿ**
   - è®¾ç½® `max_samples: 100`

---

## License

MIT License
